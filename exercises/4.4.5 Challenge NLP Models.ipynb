{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge, I will use the inaugrual speeches corpus from nltk and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "- Data cleaning / processing / language parsing\n",
    "\n",
    "- Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "\n",
    "- Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "\n",
    "- Assess your models using cross-validation and determine whether one model performed better.\n",
    "\n",
    "- Pick one of the models and try to increase accuracy by at least 5 percentage points.\n",
    "\n",
    "I will be using the first inaugural speech of Obama and Trump and attempt to have my models predict the speaker correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import inaugural\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning, Processing, and Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My fellow citizens:\n",
      "\n",
      "I stand here today humbled by the task before us, grateful for the trust you have bestowed, mindful of the sacrifices borne by our ancestors. I thank President Bush for his service to our nation, as well as the generosity and cooperation he has shown throughout this transition.\n",
      "\n",
      "Forty-four Americans have now taken the presidential oath. The words have been spoken during rising tides of prosperity and the still waters of peace. Yet, every so often the oath is taken amidst gathering clouds and raging storms. At these moments, America has carried on not simply because of the skill or vision of those in high office, but because We the People have remained faithful to the ideals of our forbearers, and true to our founding documents.\n",
      "\n",
      "So it has been. So it must be with this generation of Americans.\n",
      "\n",
      "That we are in the midst of crisis is now well understood. Our nation is at war, against a far-reaching network of violence and hatred. Our economy is badly weakened, a consequence of greed and irresponsibility on the part of some, but also our collective failure to make hard choices and prepare the nation for a new age. Homes have been lost; jobs shed; businesses shuttered. Our health care is too costly; our schools fail too many; and each day brings further evidence that the ways we use energy strengthen our adversaries and threaten our planet.\n",
      "\n",
      "These are the indicators of crisis, subject to data and statistics. Less measurable but no less profound is a sapping of confidence across our land -- a nagging fear that America's decline is inevitable, that the next generation must lower its sights.\n",
      "\n",
      "Today I say to you that the challenges we face are real. They are serious and they are many. They will not be met easily or in a short span of time. But know this, America -- they will be met.\n",
      "\n",
      "On this day, we gather because we have chosen hope over fear, unity of purpose over conflict and discord.\n",
      "\n",
      "On this day, we come to proclaim an end to the petty grievances and false promises, the recriminations and worn-out dogmas that for far too long have strangled our politics.\n",
      "\n",
      "We remain a young nation, but in the words of Scripture, the time has come to set aside childish things. The time has come to reaffirm our enduring spirit; to choose our better history; to carry forward that precious gift, that noble idea, passed on from generation to generation: the God-given promise that all are equal, all are free, and all deserve a chance to pursue their full measure of happiness.\n",
      "\n",
      "In reaffirming the greatness of our nation, we understand that greatness is never a given. It must be earned. Our journey has never been one of shortcuts or settling for less. It has not been the path for the faint-hearted -- for those who prefer leisure over work, or seek only the pleasures of riches and fame. Rather, it has been the risk-takers, the doers, the makers of things'some celebrated but more often men and women obscure in their labor, who have carried us up the long, rugged path towards prosperity and freedom.\n",
      "\n",
      "For us, they packed up their few worldly possessions and traveled across oceans in search of a new life.\n",
      "\n",
      "For us, they toiled in sweatshops and settled the West; endured the lash of the whip and plowed the hard earth.\n",
      "\n",
      "For us, they fought and died, in places like Concord and Gettysburg; Normandy and Khe Sahn.\n",
      "\n",
      "Time and again these men and women struggled and sacrificed and worked till their hands were raw so that we might live a better life. They saw America as bigger than the sum of our individual ambitions; greater than all the differences of birth or wealth or faction.\n",
      "\n",
      "This is the journey we continue today. We remain the most prosperous, powerful nation on Earth. Our workers are no less productive than when this crisis began. Our minds are no less inventive, our goods and services no less needed than they were last week or last month or last year. Our capacity remains undiminished. But our time of standing pat, of protecting narrow interests and putting off unpleasant decisions -- that time has surely passed. Starting today, we must pick ourselves up, dust ourselves off, and begin again the work of remaking America.\n",
      "\n",
      "For everywhere we look, there is work to be done. The state of our economy calls for action, bold and swift, and we will act -- not only to create new jobs, but to lay a new foundation for growth. We will build the roads and bridges, the electric grids and digital lines that feed our commerce and bind us together. We will restore science to its rightful place, and wield technology's wonders to raise health care's quality and lower its cost. We will harness the sun and the winds and the soil to fuel our cars and run our factories. And we will transform our schools and colleges and universities to meet the demands of a new age. All this we can do. All this we will do.\n",
      "\n",
      "Now, there are some who question the scale of our ambitions -- who suggest that our system cannot tolerate too many big plans. Their memories are short. For they have forgotten what this country has already done; what free men and women can achieve when imagination is joined to common purpose, and necessity to courage.\n",
      "\n",
      "What the cynics fail to understand is that the ground has shifted beneath them -- that the stale political arguments that have consumed us for so long no longer apply. The question we ask today is not whether our government is too big or too small, but whether it works -- whether it helps families find jobs at a decent wage, care they can afford, a retirement that is dignified. Where the answer is yes, we intend to move forward. Where the answer is no, programs will end. And those of us who manage the public's dollars will be held to account -- to spend wisely, reform bad habits, and do our business in the light of day -- because only then can we restore the vital trust between a people and their government.\n",
      "\n",
      "Nor is the question before us whether the market is a force for good or ill. Its power to generate wealth and expand freedom is unmatched, but this crisis has reminded us that without a watchful eye, the market can spin out of control -- the nation cannot prosper long when it favors only the prosperous. The success of our economy has always depended not just on the size of our Gross Domestic Product, but on the reach of our prosperity; on the ability to extend opportunity to every willing heart -- not out of charity, but because it is the surest route to our common good.\n",
      "\n",
      "As for our common defense, we reject as false the choice between our safety and our ideals. Our Founding Fathers, faced with perils that we can scarcely imagine, drafted a charter to assure the rule of law and the rights of man, a charter expanded by the blood of generations. Those ideals still light the world, and we will not give them up for expedience's sake. And so to all the other peoples and governments who are watching today, from the grandest capitals to the small village where my father was born: know that America is a friend of each nation and every man, woman, and child who seeks a future of peace and dignity, and we are ready to lead once more.\n",
      "\n",
      "Recall that earlier generations faced down fascism and communism not just with missiles and tanks, but with the sturdy alliances and enduring convictions. They understood that our power alone cannot protect us, nor does it entitle us to do as we please. Instead, they knew that our power grows through its prudent use; our security emanates from the justness of our cause, the force of our example, the tempering qualities of humility and restraint.\n",
      "\n",
      "We are the keepers of this legacy. Guided by these principles once more, we can meet those new threats that demand even greater effort -- even greater cooperation and understanding between nations. We will begin to responsibly leave Iraq to its people, and forge a hard-earned peace in Afghanistan. With old friends and former foes, we will work tirelessly to lessen the nuclear threat, and roll back the specter of a warming planet. We will not apologize for our way of life, nor will we waver in its defense, and for those who seek to advance their aims by inducing terror and slaughtering innocents, we say to you now that our spirit is stronger and cannot be broken; you cannot outlast us, and we will defeat you.\n",
      "\n",
      "For we know that our patchwork heritage is a strength, not a weakness. We are a nation of Christians and Muslims, Jews and Hindus -- and non-believers. We are shaped by every language and culture, drawn from every end of this Earth; and because we have tasted the bitter swill of civil war and segregation, and emerged from that dark chapter stronger and more united, we cannot help but believe that the old hatreds shall someday pass; that the lines of tribe shall soon dissolve; that as the world grows smaller, our common humanity shall reveal itself; and that America must play its role in ushering in a new era of peace.\n",
      "\n",
      "To the Muslim world, we seek a new way forward, based on mutual interest and mutual respect. To those leaders around the globe who seek to sow conflict, or blame their society's ills on the West -- know that your people will judge you on what you can build, not what you destroy. To those who cling to power through corruption and deceit and the silencing of dissent, know that you are on the wrong side of history; but that we will extend a hand if you are willing to unclench your fist.\n",
      "\n",
      "To the people of poor nations, we pledge to work alongside you to make your farms flourish and let clean waters flow; to nourish starved bodies and feed hungry minds. And to those nations like ours that enjoy relative plenty, we say we can no longer afford indifference to the suffering outside our borders; nor can we consume the world's resources without regard to effect. For the world has changed, and we must change with it.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As we consider the road that unfolds before us, we remember with humble gratitude those brave Americans who, at this very hour, patrol fa\n"
     ]
    }
   ],
   "source": [
    "obama = inaugural.raw('2009-Obama.txt')\n",
    "print(obama[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13439"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(obama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chief Justice Roberts, President Carter, President Clinton, President Bush, President Obama, fellow Americans, and people of the world: Thank you.\n",
      "\n",
      "We, the citizens of America, are now joined in a great national effort to rebuild our country and restore its promise for all of our people. Together, we will determine the course of America and the world for many, many years to come. We will face challenges, we will confront hardships, but we will get the job done.\n",
      "\n",
      "Every 4 years, we gather on these steps to carry out the orderly and peaceful transfer of power, and we are grateful to President Obama and First Lady Michelle Obama for their gracious aid throughout this transition. They have been magnificent. Thank you.\n",
      "\n",
      "Today's ceremony, however, has very special meaning. Because today we are not merely transferring power from one administration to another or from one party to another, but we are transferring power from Washington, DC, and giving it back to you, the people.\n",
      "\n",
      "For too long, a small group in our Nation's Capital has reaped the rewards of Government while the people have borne the cost. Washington flourished, but the people did not share in its wealth. Politicians prospered, but the jobs left, and the factories closed. The establishment protected itself, but not the citizens of our country. Their victories have not been your victories; their triumphs have not been your triumphs; and while they celebrated in our Nation's Capital, there was little to celebrate for struggling families all across our land.\n",
      "\n",
      "That all changes, starting right here and right now, because this moment is your moment: It belongs to you. It belongs to everyone gathered here today and everyone watching all across America. This is your day. This is your celebration. And this, the United States of America, is your country.\n",
      "\n",
      "What truly matters is not which party controls our Government, but whether our Government is controlled by the people. January 20, 2017, will be remembered as the day the people became the rulers of this Nation again. The forgotten men and women of our country will be forgotten no longer. Everyone is listening to you now.\n",
      "\n",
      "You came by the tens of millions to become part of a historic movement the likes of which the world has never seen before. At the center of this movement is a crucial conviction: that a nation exists to serve its citizens. Americans want great schools for their children, safe neighborhoods for their families, and good jobs for themselves. These are just and reasonable demands of righteous people and a righteous public.\n",
      "\n",
      "But for too many of our citizens, a different reality exists: Mothers and children trapped in poverty in our inner cities; rusted-out factories scattered like tombstones across the landscape of our Nation; an education system, flush with cash, but which leaves our young and beautiful students deprived of all knowledge; and the crime and the gangs and the drugs that have stolen too many lives and robbed our country of so much unrealized potential.\n",
      "\n",
      "This American carnage stops right here and stops right now. We are one Nation, and their pain is our pain, their dreams are our dreams, and their success will be our success. We share one heart, one home, and one glorious destiny.\n",
      "\n",
      "The oath of office I take today is an oath of allegiance to all Americans.\n",
      "\n",
      "For many decades, we've enriched foreign industry at the expense of American industry, subsidized the armies of other countries while allowing for the very sad depletion of our military. We've defended other nations' borders while refusing to defend our own and spent trillions and trillions of dollars overseas while America's infrastructure has fallen into disrepair and decay. We've made other countries rich while the wealth, strength, and confidence of our country has dissipated over the horizon.\n",
      "\n",
      "One by one, the factories shuttered and left our shores, with not even a thought about the millions and millions of American workers that were left behind. The wealth of our middle class has been ripped from their homes and then redistributed all across the world.\n",
      "\n",
      "But that is the past. And now we are looking only to the future.\n",
      "\n",
      "We, assembled here today, are issuing a new decree to be heard in every city, in every foreign capital, and in every hall of power. From this day forward, a new vision will govern our land. From this this day forward, it's going to be only America first. America first.\n",
      "\n",
      "Every decision on trade, on taxes, on immigration, on foreign affairs, will be made to benefit American workers and American families.\n",
      "\n",
      "We must protect our borders from the ravages of other countries making our products, stealing our companies, and destroying our jobs. Protection will lead to great prosperity and strength. I will fight for you with every breath in my body, and I will never, ever let you down.\n",
      "\n",
      "America will start winning again, winning like never before. We will bring back our jobs. We will bring back our borders. We will bring back our wealth. And we will bring back our dreams.\n",
      "\n",
      "We will build new roads and highways and bridges and airports and tunnels and railways all across our wonderful Nation.\n",
      "\n",
      "We will get our people off of welfare and back to work, rebuilding our country with American hands and American labor. We will follow two simple rules: Buy American and hire American.\n",
      "\n",
      "We will seek friendship and good will with the nations of the world, but we do so with the understanding that it is the right of all nations to put their own interests first. We do not seek to impose our way of life on anyone, but rather to let it shine as an exampleâwe will shineâfor everyone to follow.\n",
      "\n",
      "We will reinforce old alliances and form new ones and unite the civilized world against radical Islamic terrorism, which we will eradicate completely from the face of the Earth.\n",
      "\n",
      "At the bedrock of our politics will be a total allegiance to the United States of America, and through our loyalty to our country, we will rediscover our loyalty to each other. When you open your heart to patriotism, there is no room for prejudice. The Bible tells us, \"How good and pleasant it is when God's people live together in unity.\" We must speak our minds openly, debate our disagreements honestly, but always pursue solidarity. When America is united, America is totally unstoppable. There should be no fear: We are protected, and we will always be protected. We will be protected by the great men and women of our military and law enforcement, and most importantly, we will be protected by God.\n",
      "\n",
      "Finally, we must think big and dream even bigger. In America, we understand that a nation is only living as long as it is striving.\n",
      "\n",
      "We will no longer accept politicians who are all talk and no action, constantly complaining, but never doing anything about it. The time for empty talk is over. Now arrives the hour of action.\n",
      "\n",
      "Do not allow anyone to tell you that it cannot be done. No challenge can match the heart and fight and spirit of America. We will not fail. Our country will thrive and prosper again.\n",
      "\n",
      "We stand at the birth of a new millennium, ready to unlock the mysteries of space, to free the Earth from the miseries of disease, and to harness the energies, industries, and technologies of tomorrow. A new national pride will stir our souls, lift our sights, and heal our divisions.\n",
      "\n",
      "It's time to remember that old wisdom our soldiers will never forget: that whether we are Black or Brown or White, we all bleed the same red blood of patriots, we all enjoy the same glorious freedoms, and we all salute the same great American flag.\n",
      "\n",
      "And whether a child is born in the urban sprawl of Detroit or the windswept plains of Nebraska, they look up at the same night sky, they fill their heart with the same dreams, and they are infused with the breath of life by the same almighty Creator.\n",
      "\n",
      "So to all Americans in every city near and far, small and large, from mountain to mountain, from ocean to ocean, hear these words: You will never be ignored again. Your voice, your hopes, and your dreams will define our American destiny. And your courage and goodness and love will forever guide us along the way.\n",
      "\n",
      "Together, we will make America strong again. We will make America wealthy again. We will make America proud again. We will make America safe again.\n",
      "\n",
      "And, yes, together, we will make America great again. Thank you. God bless you, and God bless America. Thank you. God bless America.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trump = inaugural.raw('2017-Trump.txt')\n",
    "print(trump[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8449"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "obama = text_cleaner(obama)\n",
    "trump = text_cleaner(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "obama_doc = nlp(obama)\n",
    "trump_doc = nlp(trump)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words+ Features Model\n",
    "\n",
    "I will create a model that uses the Bag of Words approach, plus a few extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(My, fellow, citizens, :, I, stand, here, toda...</td>\n",
       "      <td>obama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(I, thank, President, Bush, for, his, service,...</td>\n",
       "      <td>obama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Forty, -, four, Americans, have, now, taken, ...</td>\n",
       "      <td>obama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(The, words, have, been, spoken, during, risin...</td>\n",
       "      <td>obama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Yet, ,, every, so, often, the, oath, is, take...</td>\n",
       "      <td>obama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0      1\n",
       "0  (My, fellow, citizens, :, I, stand, here, toda...  obama\n",
       "1  (I, thank, President, Bush, for, his, service,...  obama\n",
       "2  (Forty, -, four, Americans, have, now, taken, ...  obama\n",
       "3  (The, words, have, been, spoken, during, risin...  obama\n",
       "4  (Yet, ,, every, so, often, the, oath, is, take...  obama"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_sents = [[sent, 'obama'] for sent in obama_doc.sents]\n",
    "trump_sents = [[sent, 'trump'] for sent in trump_doc.sents]\n",
    "\n",
    "sentences = pd.DataFrame(obama_sents + trump_sents)\n",
    "\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to create a list of the 500 most common words.\n",
    "def bag_of_words(text):\n",
    "    \n",
    "    # Filter out punctuation and stop words.\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(500)]\n",
    "\n",
    "obama_words = bag_of_words(obama_doc)\n",
    "trump_words = bag_of_words(trump_doc)\n",
    "\n",
    "common_words = set(obama_words + trump_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "758"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a data frame with features for each word in our common word set.\n",
    "# Each value is the count of the times the word appears in each sentence.\n",
    "# Also features for sentence length, amount of punctuation per sentence,\n",
    "# and parts of speech counts\n",
    "\n",
    "def nlp_features(sentences, common_words):\n",
    "    \n",
    "    # Scaffold the data frame\n",
    "    df = pd.DataFrame(columns=common_words)\n",
    "    df['sentence'] = sentences[0]\n",
    "    df['speaker'] = sentences[1]\n",
    "    # Set empty features\n",
    "    df['sentence_length'] = np.nan\n",
    "    df['punctuation_count'] = np.nan\n",
    "    df['noun_count'] = np.nan\n",
    "    df['verb_count'] = np.nan\n",
    "    df['adjective_count'] = np.nan\n",
    "    df['adverb_count'] = np.nan\n",
    "    df['pronoun_count'] = np.nan\n",
    "    df['proper_noun_count'] = np.nan\n",
    "    df['conjunction_count'] = np.nan\n",
    "    # Initialize word counts to zer0\n",
    "    df.loc[:, common_words] = 0\n",
    "    \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    for i, sentence in enumerate(df['sentence']):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            df.loc[i, word] += 1\n",
    "        \n",
    "        # Create the other feature counts\n",
    "        df.loc[i, 'sentence_length'] = len([token for token in sentence if not token.is_punct])\n",
    "        df.loc[i, 'punctuation_count'] = len([token for token in sentence if token.is_punct])\n",
    "        df.loc[i, 'noun_count'] = len([token for token in sentence if token.pos_ == 'NOUN'])\n",
    "        df.loc[i, 'verb_count'] = len([token for token in sentence if token.pos_ == 'VERB'])\n",
    "        df.loc[i, 'conjunction_count'] = len([token for token in sentence if token.pos_ == 'CONJ' or 'ADP'])\n",
    "        df.loc[i, 'adjective_count'] = len([token for token in sentence if token.pos_ == 'ADJ'])\n",
    "        df.loc[i, 'adverb_count'] = len([token for token in sentence if token.pos_ == 'ADV'])\n",
    "        df.loc[i, 'pronoun_count'] = len([token for token in sentence if token.pos_ == 'PRON'])\n",
    "        df.loc[i, 'proper_noun_count'] = len([token for token in sentence if token.pos_ == 'PROPN'])\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measure</th>\n",
       "      <th>crisis</th>\n",
       "      <th>change</th>\n",
       "      <th>control</th>\n",
       "      <th>favor</th>\n",
       "      <th>peril</th>\n",
       "      <th>scarcely</th>\n",
       "      <th>govern</th>\n",
       "      <th>ceremony</th>\n",
       "      <th>depletion</th>\n",
       "      <th>...</th>\n",
       "      <th>speaker</th>\n",
       "      <th>sentence_length</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>noun_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>adjective_count</th>\n",
       "      <th>adverb_count</th>\n",
       "      <th>pronoun_count</th>\n",
       "      <th>proper_noun_count</th>\n",
       "      <th>conjunction_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>obama</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>obama</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>obama</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>obama</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>obama</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   measure  crisis  change  control  favor  peril  scarcely  govern  ceremony  \\\n",
       "0        0       0       0        0      0      0         0       0         0   \n",
       "1        0       0       0        0      0      0         0       0         0   \n",
       "2        0       0       0        0      0      0         0       0         0   \n",
       "3        0       0       0        0      0      0         0       0         0   \n",
       "4        0       0       0        0      0      0         0       0         0   \n",
       "\n",
       "   depletion  ...  speaker  sentence_length  punctuation_count  noun_count  \\\n",
       "0          0  ...    obama             28.0                4.0         6.0   \n",
       "1          0  ...    obama             23.0                2.0         5.0   \n",
       "2          0  ...    obama              9.0                2.0         1.0   \n",
       "3          0  ...    obama             16.0                1.0         5.0   \n",
       "4          0  ...    obama             14.0                2.0         4.0   \n",
       "\n",
       "   verb_count  adjective_count  adverb_count  pronoun_count  \\\n",
       "0         5.0              5.0           1.0            3.0   \n",
       "1         3.0              2.0           2.0            2.0   \n",
       "2         2.0              1.0           1.0            0.0   \n",
       "3         4.0              0.0           1.0            0.0   \n",
       "4         3.0              0.0           3.0            0.0   \n",
       "\n",
       "   proper_noun_count  conjunction_count  \n",
       "0                0.0               32.0  \n",
       "1                2.0               25.0  \n",
       "2                1.0               11.0  \n",
       "3                0.0               17.0  \n",
       "4                0.0               16.0  \n",
       "\n",
       "[5 rows x 769 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = nlp_features(sentences, common_words)\n",
    "\n",
    "nlp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation and Selection\n",
    "\n",
    "I will use cross validation on Gradient Boosting, Random Forest, and Logistic Regression models and select which performs best to represent the Bag of Words+ feature approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score as cvs\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = nlp.drop(columns=['sentence', 'speaker'])\n",
    "y = nlp['speaker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5       , 0.54761905, 0.6097561 , 0.6       , 0.525     ])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=1000, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "\n",
    "cvs(gbc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47619048, 0.57142857, 0.63414634, 0.6       , 0.425     ])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "\n",
    "cvs(gbc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57142857, 0.5952381 , 0.58536585, 0.525     , 0.525     ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=1000, max_depth=2, learning_rate=0.1, random_state=42)\n",
    "\n",
    "cvs(gbc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.47619048, 0.54761905, 0.63414634, 0.625     , 0.55      ])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=1000, max_depth=2, learning_rate=0.01, random_state=42)\n",
    "\n",
    "cvs(gbc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57142857, 0.54761905, 0.63414634, 0.625     , 0.575     ])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=1000, max_depth=5, random_state=42)\n",
    "\n",
    "cvs(rfc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54761905, 0.54761905, 0.56097561, 0.55      , 0.525     ])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=1000, max_depth=2, random_state=42)\n",
    "\n",
    "cvs(rfc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54761905, 0.57142857, 0.56097561, 0.55      , 0.55      ])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10000, max_depth=2, random_state=42)\n",
    "\n",
    "cvs(rfc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5952381 , 0.57142857, 0.56097561, 0.55      , 0.575     ])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10000, max_depth=3, random_state=42)\n",
    "\n",
    "cvs(rfc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57142857, 0.57142857, 0.63414634, 0.6       , 0.575     ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10000, max_depth=5, random_state=42)\n",
    "\n",
    "cvs(rfc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71428571, 0.5       , 0.68292683, 0.6       , 0.575     ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', solver='lbfgs')\n",
    "\n",
    "cross_val_score(lr, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64285714, 0.45238095, 0.70731707, 0.625     , 0.575     ])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "cross_val_score(lr, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most stable performing model for this feature set is the Random Forest Classifier with 10,000 trees and a node depth of 3 (a depth of 5 performed slightly better, but had less consistency.\n",
    "\n",
    "I will move to feature extraction through tfidf to see if I can get a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf-Idf Feature based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama = inaugural.sents('2009-Obama.txt')\n",
    "trump = inaugural.sents('2017-Trump.txt')\n",
    "\n",
    "obama_sents = []\n",
    "for sentence in obama:\n",
    "    sentence=[re.sub(r'--','',word) for word in sentence]\n",
    "    #Forming each paragraph into a string and adding it to the list of strings.\n",
    "    obama_sents.append(' '.join(sentence))\n",
    "    \n",
    "trump_sents = []\n",
    "for sentence in trump:\n",
    "    sentence=[re.sub(r'--', '', word) for word in para]\n",
    "    trump_sents.append(' '.join(sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My fellow citizens :</td>\n",
       "      <td>Obama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I stand here today humbled by the task before ...</td>\n",
       "      <td>Obama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thank President Bush for his service to our ...</td>\n",
       "      <td>Obama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Forty - four Americans have now taken the pres...</td>\n",
       "      <td>Obama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The words have been spoken during rising tides...</td>\n",
       "      <td>Obama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence speaker\n",
       "0                               My fellow citizens :   Obama\n",
       "1  I stand here today humbled by the task before ...   Obama\n",
       "2  I thank President Bush for his service to our ...   Obama\n",
       "3  Forty - four Americans have now taken the pres...   Obama\n",
       "4  The words have been spoken during rising tides...   Obama"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_sents = [[sentence, 'Obama'] for sentence in obama_sents]\n",
    "trump_sents = [[sentence, 'Trump'] for sentence in trump_sents]\n",
    "\n",
    "obama_df = pd.DataFrame(obama_sents)\n",
    "trump_df = pd.DataFrame(trump_sents)\n",
    "\n",
    "sentences = pd.concat([obama_df, trump_df])\n",
    "sentences.columns = ['sentence', 'speaker']\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 153)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_df=0.50, # drop words that occur in more than half of the sentences\n",
    "                        min_df=2, # only use words that appear at least twice\n",
    "                        stop_words='english', \n",
    "                        lowercase=True,\n",
    "                        use_idf=True,\n",
    "                        norm='l2',\n",
    "                        smooth_idf=True \n",
    "                        )\n",
    "\n",
    "features = tfidf.fit_transform(sentences.sentence).toarray()\n",
    "speaker = sentences.speaker\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now have a tf-idf vector with 153 features for all 202 of the sentences in our two speakers' speeches. I will now perform dimension reduction using SVD before putting these features through models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 86.08211742480594\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "svd = TruncatedSVD(50)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "features_lsa = lsa.fit_transform(features)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_lsa = pd.DataFrame(features_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.046199e-10</td>\n",
       "      <td>-0.014653</td>\n",
       "      <td>-0.003307</td>\n",
       "      <td>0.021139</td>\n",
       "      <td>-0.039822</td>\n",
       "      <td>0.013090</td>\n",
       "      <td>-0.025672</td>\n",
       "      <td>-0.010206</td>\n",
       "      <td>-0.039347</td>\n",
       "      <td>-0.067744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074523</td>\n",
       "      <td>-0.312874</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>-0.317145</td>\n",
       "      <td>0.015131</td>\n",
       "      <td>-0.133358</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>-0.075164</td>\n",
       "      <td>0.118999</td>\n",
       "      <td>-0.186333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.908475e-04</td>\n",
       "      <td>0.125269</td>\n",
       "      <td>-0.049826</td>\n",
       "      <td>0.173678</td>\n",
       "      <td>0.011714</td>\n",
       "      <td>-0.061401</td>\n",
       "      <td>0.024625</td>\n",
       "      <td>0.294839</td>\n",
       "      <td>0.273731</td>\n",
       "      <td>-0.197758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157111</td>\n",
       "      <td>-0.156559</td>\n",
       "      <td>0.027084</td>\n",
       "      <td>0.168092</td>\n",
       "      <td>0.323889</td>\n",
       "      <td>0.055963</td>\n",
       "      <td>0.291603</td>\n",
       "      <td>-0.103671</td>\n",
       "      <td>-0.135865</td>\n",
       "      <td>0.124915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.252687e-04</td>\n",
       "      <td>0.315220</td>\n",
       "      <td>-0.224252</td>\n",
       "      <td>-0.273194</td>\n",
       "      <td>-0.099771</td>\n",
       "      <td>-0.202050</td>\n",
       "      <td>-0.032729</td>\n",
       "      <td>-0.013078</td>\n",
       "      <td>0.025797</td>\n",
       "      <td>-0.061808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026618</td>\n",
       "      <td>0.026993</td>\n",
       "      <td>-0.001353</td>\n",
       "      <td>-0.007741</td>\n",
       "      <td>-0.000705</td>\n",
       "      <td>0.004791</td>\n",
       "      <td>-0.001914</td>\n",
       "      <td>-0.032065</td>\n",
       "      <td>0.006595</td>\n",
       "      <td>-0.021935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.885232e-06</td>\n",
       "      <td>0.032362</td>\n",
       "      <td>-0.046252</td>\n",
       "      <td>0.019306</td>\n",
       "      <td>-0.066298</td>\n",
       "      <td>0.227510</td>\n",
       "      <td>-0.087092</td>\n",
       "      <td>-0.057219</td>\n",
       "      <td>0.225864</td>\n",
       "      <td>0.143585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015019</td>\n",
       "      <td>0.057856</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>-0.041788</td>\n",
       "      <td>-0.043848</td>\n",
       "      <td>0.022843</td>\n",
       "      <td>0.015696</td>\n",
       "      <td>-0.057411</td>\n",
       "      <td>-0.105684</td>\n",
       "      <td>0.011366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.821903e-04</td>\n",
       "      <td>0.169116</td>\n",
       "      <td>-0.150595</td>\n",
       "      <td>0.099924</td>\n",
       "      <td>-0.052433</td>\n",
       "      <td>0.023431</td>\n",
       "      <td>-0.008582</td>\n",
       "      <td>-0.026819</td>\n",
       "      <td>-0.044918</td>\n",
       "      <td>0.090594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214141</td>\n",
       "      <td>0.068422</td>\n",
       "      <td>0.232494</td>\n",
       "      <td>-0.021983</td>\n",
       "      <td>-0.182088</td>\n",
       "      <td>0.074826</td>\n",
       "      <td>0.140921</td>\n",
       "      <td>0.063264</td>\n",
       "      <td>-0.154343</td>\n",
       "      <td>0.020363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0 -7.046199e-10 -0.014653 -0.003307  0.021139 -0.039822  0.013090 -0.025672   \n",
       "1  3.908475e-04  0.125269 -0.049826  0.173678  0.011714 -0.061401  0.024625   \n",
       "2  2.252687e-04  0.315220 -0.224252 -0.273194 -0.099771 -0.202050 -0.032729   \n",
       "3  1.885232e-06  0.032362 -0.046252  0.019306 -0.066298  0.227510 -0.087092   \n",
       "4  3.821903e-04  0.169116 -0.150595  0.099924 -0.052433  0.023431 -0.008582   \n",
       "\n",
       "         7         8         9   ...        40        41        42        43  \\\n",
       "0 -0.010206 -0.039347 -0.067744  ...  0.074523 -0.312874  0.000251 -0.317145   \n",
       "1  0.294839  0.273731 -0.197758  ...  0.157111 -0.156559  0.027084  0.168092   \n",
       "2 -0.013078  0.025797 -0.061808  ...  0.026618  0.026993 -0.001353 -0.007741   \n",
       "3 -0.057219  0.225864  0.143585  ...  0.015019  0.057856  0.012784 -0.041788   \n",
       "4 -0.026819 -0.044918  0.090594  ...  0.214141  0.068422  0.232494 -0.021983   \n",
       "\n",
       "         44        45        46        47        48        49  \n",
       "0  0.015131 -0.133358  0.002149 -0.075164  0.118999 -0.186333  \n",
       "1  0.323889  0.055963  0.291603 -0.103671 -0.135865  0.124915  \n",
       "2 -0.000705  0.004791 -0.001914 -0.032065  0.006595 -0.021935  \n",
       "3 -0.043848  0.022843  0.015696 -0.057411 -0.105684  0.011366  \n",
       "4 -0.182088  0.074826  0.140921  0.063264 -0.154343  0.020363  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_lsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95121951, 1.        , 1.        , 1.        , 0.975     ])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = features_lsa\n",
    "y = speaker\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=1000, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "\n",
    "cvs(gbc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10000, max_depth=3, random_state=42)\n",
    "\n",
    "cvs(rfc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.   , 1.   , 1.   , 1.   , 0.975])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', solver='lbfgs')\n",
    "\n",
    "cross_val_score(lr, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that I have created some very good models using the tf-idf features with reduced dimensions. Perhaps too good, as overfitting is now a querstion. But within the scope of the questions of predicting whether a sentence came from Obama or Trump's inaugrual speaches, I have some very well-performing models to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
